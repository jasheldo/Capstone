---
title: "Capstone Milestone Report"
author: "James Sheldon"
date: "Auguts 30, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Intent

The goal of this project is just to display that you've gotten used to working with the data and that we are on track to create a prediction algorithm.

## Assumption

1. The data is already downloaded and available in the working directory.
2. The necessary libraries are installed.

## Reading Data and Basic Analysis

### Introduction

The capstone project involves taking language data from 12 sources (four languages and three major sources each) and attempt to create a Shiny predictive language application using machine learning. This is a hot area of research right now, especially with the rise of personal assistants like [Siri](http://www.apple.com/siri) and [Alexa](https://www.amazon.com/gp/product/B00X4WHP5E/ref=sv_devicesubnav_0).

In order to get the most out of the project, many predictive language and data manipulation libraries have been loaded into R. Almost all of them are new to the user so it's worth taking the requisite time to read the syllabus and supporting documentation to get a firm handle on their capabilities and how they apply to the project.

The plan of attack is as follows:

1. Download the data. It is assumed already that the user has performed this operation and unzipped the files into a usable directory.
2. Read in the three English language data sets and perform some cursory analysis on them.
3. Capture a random subset of these data.
4. Create a corpus from the data using the [TM library](https://cran.r-project.org/web/packages/tm/index.html).
5. Use that corpus to create a Term Document Matrix (TDM).
5. From the TDM, produce a tokenized review the N-Gram frequency for N = 1, 2 and 3.
6. Plot these frequencies using standard density plots as well as the very fun text bubbles.

After performing the above review we will be in a position to outline the strategy to create the predictive model.  Much of this analysis follows the best practices outlined in the _TM_ introduction PDF.

### Libraries

Let's load the libraries. Many of these are based off the supporting text from the syllabus. Are all of them needed? Probably not. But there's no harm in loading them.

```{r libraries}
library(RWeka)
library(dplyr)
library(stringi)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
```

## Reading Data

We begin by loading the complete data set for each of the three English language data sources: blogs, twitter and news. In the process we'll use the command _iconv_ from the base package to convert each of the files from type "Latin1" to "ASCII". This makes subsequent analyses more simple using the _stringi_ package.

The news file is a bit more complex because it won't read the file completely unless we first open a connection using the _file_ command followed by _readLines_.

```{r data}
setwd("C:/Users/jsheld01/Downloads/Coursera-SwiftKey/final/en_US")
blogs <- readLines("en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
con <- file("en_US.news.txt", "rb")
news <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con); rm(con)
twitter <- readLines("en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
```

## Exploratory Analysis

The scope of the overarching project is to be able to predict a word given two preceding words. Because of his it's important to understand not only the words that appear in the data but also which pairs and triples of words appear most frequently.

To get an understanding of how this plays out we don't need to use all the data (despite it being loaded). Instead we'll take a subset of 1-3% of the supplied data, and perform the needed frequency and N-Gram analysis of this subset.

Let's begin with a review of the total data sets. Using *stri_count_words* from the _stringi_ package we will review the number of words on each line, as well as the mean and max number of words on each line of text. Our summary will also include the total number of words, characters and lines in each of the three text files.

We know these are big files. This will give us a sense for the magnitude of big-ness.

```{r explore1}

WPL=sapply(list(blogs,news,twitter),function(x) summary(stri_count_words(x))[c('Mean','Min.','Max.')])
rownames(WPL)=c('WPL_Mean','WPL_Min','WPL_Max')
stats=data.frame(
  Dataset=c("blogs","news","twitter"),      
  t(rbind(
  sapply(list(blogs,news,twitter),stri_stats_general)[c('Lines','Chars'),],
  Words=sapply(list(blogs,news,twitter),stri_stats_latex)['Words',],
  WPL)
))
head(stats)

```

## Cleaning Up & Sampling

Now that we have the data loaded and ready for processing it's time to do a little cleanup. Per the _TM_ documentation it makes sense to convert the documents to ASCII from Latin for processing purposes.

```{r sampling}
blogs <- iconv(blogs, "latin1", "ASCII", sub="")
news <- iconv(news, "latin1", "ASCII", sub="")
twitter <- iconv(twitter, "latin1", "ASCII", sub="")

set.seed(23238)
sample_data <- c(sample(blogs, round(0.02 * length(blogs),0)),
                 sample(news, round(0.02 * length(news),0)),
                 sample(twitter, round(0.02 * length(twitter),0)))
```

## Corpus Construction and Tokenization

It's time to take our sample and use it as the basis of our corpus. We create a Volatile Corpus and perform a few cleanup procedures. It should be noted that we combined our random samples into one data file. The scope of this project is to create one predictive model. To that end we are producing only one corpus.

It should be noted that we have yet to remove offensive words. For purposes of this exploratory analysis it's okay but certainly something to keep in mind as we move closer to the final goal.

```{r corpus}
corpus <- VCorpus(VectorSource(sample_data)) # Create Volatile Corpus
corpus <- tm_map(corpus, content_transformer(tolower)) # Change all characters to lower case
corpus <- tm_map(corpus, removePunctuation) # remove punctuation
corpus <- tm_map(corpus, removeNumbers) # remove numbers
corpus <- tm_map(corpus, stripWhitespace) # remove whitespace
corpus <- tm_map(corpus, PlainTextDocument) # convert the document to plain text
```

### Tokenization:

We now tokenize our corpus according to the _NGramTokenizer_ approach from the _RWeka_ package. This function gives us more control over the tokenization process and is more robust than the tokenizer provided by _TM_. Since the process does get a bit lengthy we create functions that will perform the tokenization.

After completing the tokenization on our corpus we store the results in a TDM for later processing per the _TM_ documentation.

```{r explore2}
uni_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
bi_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tri_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

uni_matrix <- TermDocumentMatrix(corpus, control = list(tokenize = uni_tokenizer))
bi_matrix <- TermDocumentMatrix(corpus, control = list(tokenize = bi_tokenizer))
tri_matrix <- TermDocumentMatrix(corpus, control = list(tokenize = tri_tokenizer))
```

### N-Gram Frequencies:

We are now in a position to review the frequency results from the TDM. We will capture, from each of the three N-Grams produced, the top fifty most frequently occurring words/phrases and produce an appropriate density plot using _ggplot2_.

```{r cleaning}

uni_corpus <- findFreqTerms(uni_matrix,lowfreq = 50)
bi_corpus <- findFreqTerms(bi_matrix,lowfreq=50)
tri_corpus <- findFreqTerms(tri_matrix,lowfreq=50)

uni_corpus_freq <- rowSums(as.matrix(uni_matrix[uni_corpus,]))
uni_corpus_freq <- data.frame(word=names(uni_corpus_freq), frequency=uni_corpus_freq)
bi_corpus_freq <- rowSums(as.matrix(bi_matrix[bi_corpus,]))
bi_corpus_freq <- data.frame(word=names(bi_corpus_freq), frequency=bi_corpus_freq)
tri_corpus_freq <- rowSums(as.matrix(tri_matrix[tri_corpus,]))
tri_corpus_freq <- data.frame(word=names(tri_corpus_freq), frequency=tri_corpus_freq)
head(tri_corpus_freq)

plot_n_grams <- function(data, title, num) {
  df2 <- data[order(-data$frequency),][1:num,] 
  ggplot(df2, aes(x = seq(1:num), y = frequency)) +
    geom_bar(stat = "identity", fill = "blue", colour = "black", width = 0.80) +
    coord_cartesian(xlim = c(0, num+1)) +
    labs(title = title) +
    xlab("Words") +
    ylab("Count") +
    scale_x_discrete(breaks = seq(1, num, by = 1), labels = df2$word[1:num]) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
}

plot_n_grams(uni_corpus_freq,"Top Unigrams",20)
plot_n_grams(bi_corpus_freq,"Top Bigrams",20)
plot_n_grams(tri_corpus_freq,"Top Trigrams",20)
wordcloud(corpus, max.words = 75, random.order = FALSE, colors = brewer.pal(8,"Dark2"))
```



## Prediction & Shiny:

This is a very resource intensive project. The skills and techniquest learned in the Data Science Specialization have certainly helped to build a foundation to tackle this project but it's going to take a lot of additional research and learning to reach a usable app for predictive modeling in any setting.

## System Info

```{r Session}
sessionInfo()
```